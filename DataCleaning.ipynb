{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77408ce4-9098-49da-9a65-d670fac1ac8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\adity\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\adity\\anaconda3\\lib\\site-packages (19.0.0)\n",
      "Collecting fastparquet\n",
      "  Downloading fastparquet-2024.11.0-cp313-cp313-win_amd64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\adity\\anaconda3\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Collecting cramjam>=2.3 (from fastparquet)\n",
      "  Downloading cramjam-2.11.0-cp313-cp313-win_amd64.whl.metadata (681 bytes)\n",
      "Requirement already satisfied: fsspec in c:\\users\\adity\\anaconda3\\lib\\site-packages (from fastparquet) (2025.3.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\adity\\anaconda3\\lib\\site-packages (from fastparquet) (24.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\adity\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading fastparquet-2024.11.0-cp313-cp313-win_amd64.whl (673 kB)\n",
      "   ---------------------------------------- 0.0/673.3 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 262.1/673.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 673.3/673.3 kB 6.2 MB/s eta 0:00:00\n",
      "Downloading cramjam-2.11.0-cp313-cp313-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 17.5 MB/s eta 0:00:00\n",
      "Installing collected packages: cramjam, fastparquet\n",
      "\n",
      "   ---------------------------------------- 2/2 [fastparquet]\n",
      "\n",
      "Successfully installed cramjam-2.11.0 fastparquet-2024.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas pyarrow fastparquet openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "142a9785-3d1f-4a46-bb29-765bea9ddf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 - Imports and paths\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "SRC = \"Main.csv\"                # input\n",
    "DST = \"Main_clean.csv\"          # cleaned output\n",
    "DICT = \"Main_data_dictionary.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb2b2561-c7af-4d79-9d7e-2ee60a671396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows x Cols: (187260, 30)\n"
     ]
    }
   ],
   "source": [
    "# Step 1 - Load everything as string to avoid mixed-type surprises\n",
    "na_tokens = [\"\", \"NA\", \"NaN\", \"null\", \"None\", \"N/A\"]\n",
    "df_raw = pd.read_csv(\n",
    "    SRC,\n",
    "    dtype=str,\n",
    "    keep_default_na=True,\n",
    "    na_values=na_tokens\n",
    ")\n",
    "\n",
    "# Make a working copy - do not drop rows\n",
    "df = df_raw.copy()\n",
    "print(\"Rows x Cols:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3895580d-e85b-4218-bea3-805e0261eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Whitespace normalization on all cells, preserve content\n",
    "def normalize_str_col(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(\"string\")\n",
    "    s = s.str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    # keep blank as <NA> if truly empty after trim\n",
    "    s = s.replace({\"\": pd.NA})\n",
    "    return s\n",
    "\n",
    "for c in df.columns:\n",
    "    df[c] = normalize_str_col(df[c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1416d1d-e459-4ba6-b7a5-7027b6bbf886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned column names:\n",
      "['Event', 'Term Code', 'Calendar Year', 'Academic Year', 'Fiscal Year', 'Degree Type', 'Degree Level Code', 'Degree Level', 'Degree', 'Program Code', 'Program', 'Major Code', 'Major', 'College Code', 'College', 'Department Code', 'Department', 'Campus Code', 'Campus', 'Gender Code', 'Gender', 'Race And Ethnicity Code', 'Race And Ethnicity', 'Citizenship Status Code', 'Citizensihp Status', 'Residency Code', 'Residency', 'Time Status Code', 'Time Status', 'Total']\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Clean column names for Power BI\n",
    "# - remove leading \"Graduate_School_Dashboard_Dataset_\"\n",
    "# - replace '_' with ' '\n",
    "# - title case\n",
    "# - ensure uniqueness\n",
    "\n",
    "def clean_colname(name: str) -> str:\n",
    "    name = re.sub(r\"^Graduate_School_Dashboard_Dataset_\", \"\", name)\n",
    "    name = name.replace(\"_\", \" \")\n",
    "    name = name.strip()\n",
    "    name = re.sub(r\"\\s+\", \" \", name)\n",
    "    return name.title()\n",
    "\n",
    "new_names = [clean_colname(c) for c in df.columns]\n",
    "\n",
    "# ensure uniqueness\n",
    "seen = {}\n",
    "final_names = []\n",
    "for n in new_names:\n",
    "    if n not in seen:\n",
    "        seen[n] = 1\n",
    "        final_names.append(n)\n",
    "    else:\n",
    "        seen[n] += 1\n",
    "        final_names.append(f\"{n} {seen[n]}\")\n",
    "\n",
    "rename_map = dict(zip(df.columns, final_names))\n",
    "df.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "print(\"Cleaned column names:\")\n",
    "print(list(df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1612e8e1-ff1d-4f47-a6c4-d7c8b6e97be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Protect identifiers and format Term Code\n",
    "# Treat columns containing these tokens as identifiers to keep as text\n",
    "ID_TOKENS = (\"code\", \"id\", \"uid\", \"guid\")\n",
    "\n",
    "# Identify Term Code columns after renaming (e.g., \"Term Code\")\n",
    "term_code_cols = [c for c in df.columns if re.search(r\"\\bterm\\s*code\\b\", c, flags=re.I)]\n",
    "\n",
    "# Left-pad Term Code to 6 digits, but keep as text\n",
    "for c in term_code_cols:\n",
    "    df[c] = df[c].astype(\"string\")\n",
    "    df[c] = df[c].where(df[c].isna(), df[c].str.replace(r\"\\D\", \"\", regex=True))  # strip non-digits only if present\n",
    "    df[c] = df[c].where(df[c].isna(), df[c].str.zfill(6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f80268f4-eb61-4ca6-be11-1d9ef8dfaa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric-like columns inferred: ['Calendar Year', 'Academic Year', 'Fiscal Year', 'Total'] \n"
     ]
    }
   ],
   "source": [
    "# Step 5 - Infer numeric-like columns safely while preserving ID/code fields\n",
    "def is_identifier(colname: str) -> bool:\n",
    "    return any(tok in colname.lower() for tok in ID_TOKENS)\n",
    "\n",
    "NUMERIC_THRESHOLD = 0.80  # at least 80 percent of non-null values must be numeric to coerce\n",
    "\n",
    "numeric_like = []\n",
    "for c in df.columns:\n",
    "    if is_identifier(c):\n",
    "        continue\n",
    "    s = df[c]\n",
    "    if s.isna().all():\n",
    "        continue\n",
    "    # Attempt numeric coercion\n",
    "    coerced = pd.to_numeric(s, errors=\"coerce\")\n",
    "    non_null = s.notna().sum()\n",
    "    numeric_ratio = coerced.notna().sum() / max(1, non_null)\n",
    "    if numeric_ratio >= NUMERIC_THRESHOLD:\n",
    "        numeric_like.append(c)\n",
    "\n",
    "print(\"Numeric-like columns inferred:\", numeric_like[:10], \"...\" if len(numeric_like) > 10 else \"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c17d64a9-d7d1-49fb-9588-c2fccceed76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 - Cast numeric-like columns and fill missing numerics with 0\n",
    "for c in numeric_like:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Explicit handling for Year columns - keep numeric\n",
    "year_cols = [c for c in df.columns if re.search(r\"\\byear\\b\", c, flags=re.I)]\n",
    "for c in year_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Fill numeric NaN with 0 and downcast to Int64 if values are whole numbers\n",
    "for c in numeric_like + year_cols:\n",
    "    if c in df.columns:\n",
    "        ser = df[c]\n",
    "        ser = ser.fillna(0)\n",
    "        # Downcast to Int64 if all values are whole numbers\n",
    "        as_int = pd.Series(np.floor(ser) == ser)\n",
    "        if bool(as_int.min()):  # all True\n",
    "            df[c] = ser.astype(\"Int64\")\n",
    "        else:\n",
    "            df[c] = ser.astype(\"Float64\")\n",
    "\n",
    "# For all remaining non-numeric columns, replace missing with empty string\n",
    "for c in df.columns:\n",
    "    if c not in numeric_like and c not in year_cols:\n",
    "        df[c] = df[c].astype(\"string\").fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6b44f60-4cbd-476b-a1ea-d7ba09203f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows preserved and numeric NaNs filled with 0.\n"
     ]
    }
   ],
   "source": [
    "# Step 7 - Assertions to guarantee row preservation and no NaN numerics\n",
    "assert df.shape[0] == df_raw.shape[0], \"Row count changed - not allowed\"\n",
    "nan_numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and df[c].isna().any()]\n",
    "assert len(nan_numeric_cols) == 0, f\"Numeric cols with NaN remain: {nan_numeric_cols}\"\n",
    "\n",
    "print(\"Rows preserved and numeric NaNs filled with 0.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59582342-012d-445e-be3c-0933461ff425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data dictionary: Main_data_dictionary.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 8 - Optional data dictionary for documentation\n",
    "data_dict = pd.DataFrame({\n",
    "    \"Column\": df.columns,\n",
    "    \"Dtype\": [str(df[c].dtype) for c in df.columns],\n",
    "    \"NonNull\": [int(df[c].notna().sum()) for c in df.columns],\n",
    "    \"Null\": [int(df[c].isna().sum()) for c in df.columns],\n",
    "    \"Distinct\": [int(df[c].nunique(dropna=True)) for c in df.columns]\n",
    "})\n",
    "data_dict.to_csv(DICT, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Wrote data dictionary:\", DICT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0e9ba81-96eb-43c7-b785-a646cbfbd3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote cleaned file: Main_clean.csv | shape: (187260, 30)\n"
     ]
    }
   ],
   "source": [
    "# Step 9 - Export cleaned dataset for Power BI\n",
    "# UTF-8 with BOM avoids odd character issues in some Windows tools\n",
    "df.to_csv(DST, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Wrote cleaned file:\", DST, \"| shape:\", df.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
